# Local Files Inventory - NOT in GitHub Repository

> **Purpose**: This document lists all files and directories that exist in the local development environment but are **excluded from the GitHub repository**. When setting up a new VM, you will need to obtain or regenerate these files.

---

## Quick Summary

| Category | Size | Source |
|----------|------|--------|
| Raw Trade Data (TRADES-DATA/) | ~13 GB | AWS S3 / Tardis.dev |
| Parquet Files (data/*_parquet/) | ~2.7 GB | Generated from raw data |
| DuckDB Database | ~247 MB | Generated by application |
| Jesse Framework | ~1.1 GB | Docker / GitHub |
| Misc (logs, temp, models) | Variable | Generated at runtime |

---

## 1. Raw Trade Data (`TRADES-DATA/`)

**Total Size**: ~13 GB

These are the raw market data files downloaded from exchanges or data providers.

### Directory Structure

```
TRADES-DATA/
├── BTC-USDT-BINANCE/                          # 11 GB
│   └── *.csv                                   # Binance spot trades (Sep-Oct 2025)
│
├── BTC-USDT-TRADES-BINANCE-FUTURES/           # 507 MB
│   └── *.csv                                   # Binance futures trades
│
├── BTC-USDT-BOOKDEPTH-BINANCE-FUTURES/        # 4.3 MB
│   └── *.data                                  # Binance futures order book
│
├── BTC-USDT-TRADES-BYBIT-FUTURES/             # 67 MB
│   └── BTCUSDT2025-11-28.csv.gz               # Bybit trades (1.96M trades)
│
└── BTC-USDT-BOOKDEPTH-BYBIT-FUTURES/          # 1.2 GB
    └── 2025-11-28_BTCUSDT_ob200.data          # Bybit order book (83K snapshots)
```

### How to Obtain

**Option 1: AWS S3 (if you have access)**
```bash
aws s3 sync s3://your-bucket/TRADES-DATA ./TRADES-DATA
```

**Option 2: Tardis.dev**
- Binance/Bybit historical data can be purchased from https://tardis.dev/
- Download the appropriate datasets and extract to `TRADES-DATA/`

**Option 3: CCXT Live Collection**
- For smaller datasets, you can collect data live using the application's ingestion features
- This takes time but doesn't require external data sources

---

## 2. Parquet Data Files (`data/*_parquet/`)

**Total Size**: ~2.7 GB

These are processed/optimized versions of the raw data, stored in Apache Parquet format for fast DuckDB queries.

### Directory Structure

```
data/
├── trades_parquet/              # 2.6 GB - Binance spot trades
│   └── *.parquet
│
├── bybit_trades_parquet/        # 85 MB - Bybit futures trades
│   └── *.parquet
│
└── bybit_orderbook_parquet/     # 41 MB - Bybit order book snapshots
    └── *.parquet
```

### How to Regenerate

**Prerequisite**: You must have the raw data in `TRADES-DATA/` first.

```bash
# Activate virtual environment
source .venv/bin/activate

# Convert Binance trades
python -m app.ingestion.trades convert

# Convert Bybit trades
python -m app.ingestion.bybit_trades convert

# Convert Bybit order book
python -m app.ingestion.bybit_orderbook convert
```

---

## 3. Database Files

### DuckDB (`tradecore.duckdb`)

**Size**: ~247 MB

Main analytics database containing:
- Aggregated candle data
- HMM model results
- Processed market data

**How to Regenerate**: Created automatically when the application runs and ingests data.

### SQLite Databases

| File | Size | Purpose |
|------|------|---------|
| `data/jobs.db` | ~936 KB | Job tracking for ingestion tasks |
| `data/orderbook.db` | ~12 KB | Live order book snapshots |
| `data/backtests.db` | ~28 KB | Backtest results storage |
| `data/experiments.db` | ~12 KB | ML experiment tracking |

**How to Regenerate**: Created automatically when features are used.

---

## 4. Kraken Data Lake (`data/lake/`)

**Size**: Variable (depends on how much data has been ingested)

```
data/lake/
└── kraken/
    └── BTC-USD/
        └── 1m/
            └── year=2025/
                └── part-*.parquet
```

This is historical Kraken data ingested through the Fetch History page.

**How to Regenerate**:
1. Start the API server: `bash scripts/run_api.sh`
2. Open http://localhost:8001/
3. Use the Fetch History UI to download data from Kraken

---

## 5. Jesse Trading Framework (`Jesse/`)

**Size**: ~1.1 GB

A complete Jesse trading framework installation with Docker support.

### Contents

```
Jesse/
└── tradecore-jesse/
    ├── docker-compose.yml
    ├── strategies/
    ├── storage/
    └── ...
```

### How to Set Up

**Option 1: Clone Jesse**
```bash
# Get Jesse framework
git clone https://github.com/jesse-ai/jesse-docker.git Jesse/tradecore-jesse
```

**Option 2: Use the run script**
```bash
# After cloning, use our convenience script
bash scripts/run_jesse.sh up
```

**Access**:
- Dashboard: http://localhost:9000 (password: `tradecore`)
- Jupyter: http://localhost:8888

---

## 6. ML/RL Artifacts

### Models (`models/`)

**Size**: Variable

Trained reinforcement learning models and checkpoints.

```
models/
└── *.zip                  # Stable Baselines 3 model files
```

**How to Regenerate**: Train new models using the Control Panel (`/control-panel`).

### Runs (`runs/`)

**Size**: Variable

TensorBoard logs and experiment artifacts.

```
runs/
└── experiment_*/
    └── events.out.tfevents.*
```

**How to Regenerate**: Generated during model training.

---

## 7. RunPod Artifacts

### Submissions (`runpod_submissions/`)

RunPod job submission payloads and results.

### Payload Files

```
runpod_payload.json      # Current RunPod job payload
tmp_payload.json         # Temporary payload file
```

---

## 8. Virtual Environment (`.venv/`)

**Size**: ~500 MB - 1 GB

Python virtual environment with all dependencies.

**How to Recreate**:
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .

# Additional dependencies not in pyproject.toml:
pip install apscheduler hmmlearn duckdb pandas_ta
```

---

## 9. Log Files (`*.log`)

| File | Purpose |
|------|---------|
| `server.log` | Uvicorn/FastAPI server logs |
| `backend.log` | Backend service logs |
| `aws_cp.log` | AWS S3 copy operation logs |
| `aws_sync.log` | AWS S3 sync operation logs |

These are generated at runtime and don't need to be preserved.

---

## 10. Temporary Files

### `tmp/`

Scratch directory for temporary files. Empty or can be safely deleted.

### `VolumeCore_latest/`

Local S3 mirror - can be re-synced from AWS if needed.

---

## New VM Setup Checklist

When setting up a new VM, follow this order:

### Step 1: Clone Repository
```bash
git clone https://github.com/Sokalledcoder/TotalCore.git
cd TotalCore
```

### Step 2: Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
pip install apscheduler hmmlearn duckdb pandas_ta
```

### Step 3: Obtain Raw Data
- Download `TRADES-DATA/` from your backup source (AWS S3, Tardis.dev, etc.)
- Or start fresh with live data collection

### Step 4: Generate Parquet Files
```bash
python -m app.ingestion.trades convert
python -m app.ingestion.bybit_trades convert
python -m app.ingestion.bybit_orderbook convert
```

### Step 5: Start Server
```bash
bash scripts/run_api.sh
```

### Step 6: Verify
```bash
curl http://localhost:8001/
# Should return {"status":"ok","server":"TradeCore API",...}
```

### Step 7 (Optional): Set Up Jesse
```bash
bash scripts/run_jesse.sh up
# Access at http://localhost:9000
```

---

## Data Sources Reference

| Data Type | Source | URL |
|-----------|--------|-----|
| Binance Spot Trades | Tardis.dev or Binance API | https://tardis.dev/ |
| Binance Futures | Tardis.dev | https://tardis.dev/ |
| Bybit Futures | Tardis.dev or Bybit API | https://tardis.dev/ |
| Kraken Data | CCXT (live via app) | Built into TradeCore |

---

## File Size Summary

| Category | Size |
|----------|------|
| **Must Have** (raw data) | ~13 GB |
| **Generated** (parquet) | ~2.7 GB |
| **Generated** (databases) | ~250 MB |
| **Optional** (Jesse) | ~1.1 GB |
| **Optional** (models/runs) | Variable |

**Total Minimum for Full Functionality**: ~16 GB

---

*Last Updated: November 30, 2025*
